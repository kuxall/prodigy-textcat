{"text": "Name: edith edith"}
{"text": "E-Mail: edith.edith@gmail.com"}
{"text": "Address: Khartoum, Sudan"}
{"text": "Github: https://github.com/edith"}
{"text": "LinkedIn: https://linkedin.com/edith"}
{"text": "Phone No. 278954244639"}
{"text": ""}
{"text": ""}
{"text": "PROFESSIONAL SUMMARY:\t\t\t\t\t\t\t\t\t"}
{"text": ""}
{"text": "Professional qualified Data Scientist/Data Analyst with over 6+ years of experience in Data Science and Analytics including Machine Learning, Data Mining and Statistical Analysis "}
{"text": "Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modelling and data visualization with large data sets of structured and unstructured data "}
{"text": "Experienced with machine learning algorithm such as logistic regression, random forest, Xgboost, KNN, SVM, neural network, linear regression, lasso regression and k-means "}
{"text": "Strong skills in statistical methodologies such as A/B test, experiment design, hypothesis test, ANOVA "}
{"text": "Used the version control tools like Git 2.X "}
{"text": "Passionate about gleaning insightful information from massive data assets and developing a culture of sound, data-driven decision making. "}
{"text": "Ability to maintain a fun, casual, professional and productive team atmosphere "}
{"text": "Experienced the full software life cycle in SDLC, Agile and Scrum methodologies. "}
{"text": "Skilled in Advanced Regression Modelling, Correlation, Multivariate Analysis, Model Building, Business Intelligence tools and application of Statistical Concepts. "}
{"text": "Proficient in Predictive Modelling, Data Mining Methods, Factor Analysis, ANOVA, Hypothetical testing, normal distribution and other advanced statistical and econometric techniques. "}
{"text": "Developed predictive models using Decision Tree, Random Forest, Na\u00efve Bayes, Logistic Regression, Cluster Analysis, and Neural Networks. "}
{"text": "Experienced in Machine Learning and Statistical Analysis with Python Scikit-Learn. "}
{"text": "Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib, Numpy, Scipy and Pandas for data analysis. "}
{"text": "Worked with complex applications such as R, SAS, Matlab and SPSS to develop neural network, cluster analysis. "}
{"text": "Understanding on Hadoop MapReduce, or any big data frameworks is added advantage.\u00a0\nStatistical skills."}
{"text": "Experience in implementing data analysis with various analytic tools, such as Anaconda 4.0JupiterNotebook 4.X, R 3.0 and Excel "}
{"text": "Solid ability to write and optimize diverse SQL queries, working knowledge of RDBMS like SQLServer2008, NoSQL databases like MongoDB3.2 "}
{"text": "Strong experience in Big data technologies like Spark 1.6, Spark sql, Pyspark, Hadoop 2.X, HDFS, Hive 1.X "}
{"text": "Experience in visualization tools like, Tableau9.X, 10.X for creating dashboards "}
{"text": "Excellent understanding Agile and Scrum development methodology, \u00a0"}
{"text": "Proficiency in application of statistical prediction modeling, machine learning classification techniques and / or econometric forecasting techniques\u00a0"}
{"text": "Proficiency in various type of optimization, Market Mix modeling, Segmentation, Time Series, Price Promo models etc.\u00a0"}
{"text": "Familiar with systems programming language, such as Java,\u00a0Python\u00a0and C++"}
{"text": "Experience in the application of Neural Network, Support Vector Machines (SVM), and Random Forest."}
{"text": "Identifies/creates the appropriate algorithm to discover patterns, validate their findings using an experimental and iterative approach.\u00a0"}
{"text": "Applies advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems.\u00a0Closely works with product managers, Service development managers, and product development team in productizing the algorithms developed.\u00a0"}
{"text": "Experience in designing star schema, Snow flake schema for\u00a0Data\u00a0Warehouse, ODS architecture. "}
{"text": "Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards, Storyline on web and desktop platforms. \u00a0"}
{"text": "Experience in working with relational databases (Teradata, Oracle) with advanced SQL programming skills"}
{"text": "In-depth knowledge of statistical procedures that are applied in Supervised / Unsupervised problems\u00a0"}
{"text": "Basic-Intermediate level proficiency in SAS (Base SAS, Enterprise Guide, Enterprise Miner) & in UNIX\u00a0"}
{"text": "Track record of applying machine learning techniques to marketing and merchandizing ideas\u00a0"}
{"text": "Experience in Big Data platforms like Hadoop platforms (Map-R, Hortonworks & others) , Aster and Graph Databases.\u00a0"}
{"text": "Excellent communication skills (verbal and written) to communicate with clients and team, prepare + deliver effective presentations."}
{"text": "Mapping and tracing\u00a0data\u00a0from system to system in order to establish\u00a0data\u00a0hierarchy and\u00a0lineage."}
{"text": "Using Data Lineage and reverse engineering as a way to track back errors in data till the data source."}
{"text": ""}
{"text": "EDUCATION:"}
{"text": ""}
{"text": "Bachelor of Computer Science."}
{"text": ""}
{"text": "TOOLS AND TECHNOLOGIES:"}
{"text": ""}
{"text": ""}
{"text": ""}
{"text": ""}
{"text": "PROFESSIONAL EXPERIENCE:\t"}
{"text": ""}
{"text": ""}
{"text": "Description:"}
{"text": "EDF Trading Limited, together with its subsidiaries, operates in the power, gas, freight, liquefied natural gas (LNG), freight, and carbon emission credits markets in Europe, Asia, and North America. It engages in generation, sale, and wholesale of electricity and gas, as well as in offshore production, transportation, storage, and wholesale trading activities"}
{"text": ""}
{"text": "   Responsibilities:"}
{"text": ""}
{"text": "Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories. "}
{"text": "Developed Spark/Scala, Python for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data. "}
{"text": "Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection. "}
{"text": "Analyze traffic patterns by calculating autocorrelation with different time lags. "}
{"text": "Ensured that the model has low False Positive Rate. "}
{"text": "Developed entire frontend and backend modules using Python on Django Web Framework. "}
{"text": "Implemented the presentation layer with HTML, CSS and JavaScript. "}
{"text": "Involved in writing stored procedures using Oracle. "}
{"text": "Involved in analysis of Business requirement, Design and Development of High level and Low level designs, Unit and Integration testing."}
{"text": "Designed & Implemented the next generation Lambda Architecture to manage Real Time/Micro Batch/Batch Analytics use cases as well as IOT use cases using Spark Core /Spark Streaming/Kafka/NIFI-Hortonworks Data Flow/Hortonworks HDP"}
{"text": "Designed and created highly scalable and sub second response time data access patterns like Pivotal HAWQ/HDP, Spark SQL/HDFS/S3 & MPP like Redshift."}
{"text": "Created roadmap, strategies & implementations around data governance, data lineage, data tagging, metadata management, security cutting across data lake platform using Apache Atlas/Waterline Data."}
{"text": "Lead the effort on managing/designing/architecting/implementing use cases for the Data Lake initiative and do presentations with C-level Executives to build confidence, capability and to drive data analytics vision for the future"}
{"text": "Lead & Implemented various predictive analytics POC using Data Science/Machine Learning algorithms(R, Python, Scala, SparkR, Spark MLib ) to demonstrate capabilities around use cases like Diet Coke Market Share reduction/Shoping Basket Analysis/Customer "}
{"text": "Segmentation/Upselling/Product Mix/Trade Promotions/Channel Optimizations in the US market and build different predictive models for future forecasting."}
{"text": "Lead the efforts to introduce the concept of Data Lake Architecture so that data from various sources like Teradata/Social Media/Sensor Data/Survey Data and Unstructured Data can be stored and process in Hadoop/Spark and provide a road map to provide timelines and steps to replace Data warehouse/Data Marts."}
{"text": "Performed Exploratory Data Analysis and Data Visualizations using R,  and Tableau. "}
{"text": "Perform a proper EDA, Univariate and bi-variate analysis to understand the intrinsic effect/combined."}
{"text": "Worked with several R packages including knitr, dplyr, SparkR, CausalInfer, spacetime."}
{"text": "Interacted with the other departments to understand and identify dataneeds and requirements."}
{"text": ""}
{"text": "Environment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark."}
{"text": ""}
{"text": ""}
{"text": "Description:"}
{"text": "Amneal Pharmaceuticals LLC develops and produces generic drugs. It offers buprenorphine and naloxone tablets, ear drops, scalp and body oils, liquid syrups, folic acid tablets, oral solids, nasal sprays, and more. The company offers its products to customers and patients worldwide."}
{"text": ""}
{"text": "Responsibilities:"}
{"text": ""}
{"text": "Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure. "}
{"text": "Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machine learning algorithms. "}
{"text": "Installed and used CaffeDeepLearning Framework "}
{"text": "Worked on different data formats such as JSON, XML and performed machine learning algorithms in Python. "}
{"text": "Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 9.7 "}
{"text": "Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis. "}
{"text": "Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects, Power BI and Smart View. "}
{"text": "Implemented Agile Methodology for building an internal application. "}
{"text": "Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems. "}
{"text": "Good knowledge of Hadoop Architecture and various components such as HDFS, JobTracker, Task Tracker, NameNode, DataNode, Secondary NameNode, and MapReduce concepts. "}
{"text": "As Architect delivered various complex OLAP databases/cubes, scorecards, dashboards and reports. "}
{"text": "Programmed a utility in Python that used multiple packages (scipy, numpy, pandas) "}
{"text": "Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes."}
{"text": "Utilized\u00a0SQL,\u00a0NoSQL databases,\u00a0Python\u00a0programing\u00a0and API interaction"}
{"text": "Worked on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005"}
{"text": "Predictive modeling using state-of-the-art methods"}
{"text": "Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.\u00a0"}
{"text": "Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.\u00a0"}
{"text": "Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics"}
{"text": "Proven experience building sustainable and trustful relationships with senior leaders."}
{"text": ""}
{"text": "Environment: Regression, Logistic regression, Hadoop, Teradata, OLTP, Unix, Python, MLLib, SAS, random forest, OLAP, HDFS, NLTK, SVM, JSON and XML"}
{"text": ""}
{"text": ""}
{"text": "Description: Acuity Insurance is an insurance company with headquarters in Sheboygan, Wisconsin. The company is the 57th largest insurer in the United States."}
{"text": "      "}
{"text": "Responsibilities:"}
{"text": ""}
{"text": "Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs. "}
{"text": "Configured Hadoop cluster with Name node and slaves and formatted HDFS. "}
{"text": "Used Oozie workflow engine to run multiple Hive and Pig jobs."}
{"text": "Launching AmazonEC2 Cloud Instances using Amazon Images (Linux) and Configuring launched instances with respect to specific applications. "}
{"text": "Exported the result set from Hive to MySQL using Sqoop after processing the data. "}
{"text": "Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior. "}
{"text": "Used Hive to partition and bucket data. "}
{"text": "Performed MapReduce Programs those are running on the cluster. "}
{"text": "Developed multiple MapReduce jobs for data cleaning and preprocessing. "}
{"text": "Analyzed the partitioned and bucketed data and compute various metrics for reporting. "}
{"text": "Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume. "}
{"text": "Sequenced, assembled and annotated the transcriptome (51 base paired reads, millions of data points)."}
{"text": "Created Perl programs for combining and analyzing transcriptome and small RNA datasets to identify novel and conserved micro RNAs"}
{"text": "Established the core bioinformatics infrastructure for the university department."}
{"text": "Developed Hive queries for Analysis across different banners. "}
{"text": "Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded to database. "}
{"text": "Wrote Pig Scripts to perform ETL procedures on the data in HDFS. "}
{"text": "Created HBase tables to store various data formats of data coming from different portfolios. "}
{"text": "Worked on improving performance of existing Hive Queries. "}
{"text": "Leveraged working with cloud computation (AWS, UC Farm cluster), thus reducing the establishment costs"}
{"text": "Provided leadership and support to colleagues and research teams in data analysis and techniques at UC Davis and with collaborative teams"}
{"text": ""}
{"text": "Environment:Linux, Hadoop, Python, MapReduce, HDFS, Python, GIT, Hive, SQL, Pig, Scoop, Flume, AWS, EC2, Twitter API, Oozie"}
{"text": ""}
{"text": ""}
{"text": "Client: Conair Corporation, Connecticut, U.S\t\t\t      \t                            Oct 2012- Dec 2013"}
{"text": "Role: Data Scientist"}
{"text": ""}
{"text": "Description: Conair Corporation is an American company which sells small appliances, personal care products, and health and beauty products for both professionals and consumers. It was founded in 1959 and has since expanded to include ten product divisions."}
{"text": ""}
{"text": "Responsibilities:"}
{"text": ""}
{"text": "Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats "}
{"text": "Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server. "}
{"text": "Wrote simple and advanced SQL queries and scripts to create standard and ad hoc reports for senior managers. "}
{"text": "Collaborated the\u00a0data\u00a0mapping document from source to target and the\u00a0data\u00a0quality assessments for the source\u00a0data.\u00a0"}
{"text": "Used Expert level understanding of different databases in combinations for Data extraction and loading, joining data extracted from different databases and loading to a specific database. "}
{"text": "Co-ordinate with various business users, stakeholders and SME to get Functional expertise, design and business test scenarios review, UAT participation and validation of financial data. "}
{"text": "Manipulating/mining data from database tables (Redshift, Oracle, Data Warehouse)\u00a0"}
{"text": "Create automated metrics using complex databases."}
{"text": "Providing analytical network support to improve quality and standard work result."}
{"text": "Root cause research to identify process breakdowns within departments and providing data through use of various skill sets to find solutions to breakdown"}
{"text": "Foster culture of continuous engineering improvement through mentoring, feedback, and metrics"}
{"text": "Broad knowledge of programming, and scripting (especially in R / Java / Python)"}
{"text": "Implemented Event Task for execute Application Automatically."}
{"text": "Troubleshot and resolved bugs in .NET applications to ensure optimal development environment."}
{"text": "Created views, queries, and data warehouse reports using SSRS providing management with financial information from SQL Server production databases. "}
{"text": "Involved in developing Patches & Updates Module."}
{"text": ""}
{"text": "Environment: Machine learning, AWS, MS Azure, Cassandra, Spark, HDFS, Hive, Pig, Linux, Python (Scikit-Learn/Scipy/Numpy/Pandas), R, SAS, SPSS, Mysql, Eclipse, PL/SQL, SQL connector, Tableau."}
{"text": "."}